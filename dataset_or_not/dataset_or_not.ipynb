{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FiYdWV7Ar_I"
      },
      "source": [
        "Copyright 2021 Google LLC.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYrkvwgEJoZ"
      },
      "source": [
        "# Assessing the veracity of semantic markup for dataset pages\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-research/google-research/blob/master/dataset_or_not/dataset_or_not.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-research/google-research/tree/master/dataset_or_not/dataset_or_not.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-FjzADgGR5j"
      },
      "source": [
        "## About this Colab\n",
        "This is a companion Colab for the paper\n",
        "\n",
        "[Dataset or Not? A study on the veracity of semantic markup for dataset pages]()\\\n",
        "*Tarfah Alrashed, Dimitris Paparas, Omar Benjelloun, Ying Sheng, and Natasha Noy*\n",
        "\n",
        "It contains python code for training the two main models from the paper, using the [Veracity of schema.org for datasets (labeled data)](https://www.kaggle.com/googleai/veracity-of-schemaorg-for-datasets-labeled-data) dataset.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before continuing, download and unzip the [Veracity of schema.org for datasets (labeled data)](https://www.kaggle.com/googleai/veracity-of-schemaorg-for-datasets-labeled-data) dataset to your computer.\n",
        "\n",
        "## Note regarding *prominent terms*\n",
        "\n",
        "The released dataset and the code in this notebook do not contain the *prominent terms* feature mentioned in the paper. This is because that feature is extracted using proprietary code that cannot be released. The interested reader can replicate this feature extraction using the model proposed in [this paper](https://arxiv.org/abs/1805.01334)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZwERv32GV4z"
      },
      "source": [
        "#Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OhdmjXkkqGR7",
        "outputId": "44a72e81-a111-4ec8-f3ef-00524e5bfa15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adanet in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: absl-py<1.0,>=0.7 in /usr/local/lib/python3.11/dist-packages (from adanet) (0.15.0)\n",
            "Requirement already satisfied: six<2.0,>=1.11 in /usr/local/lib/python3.11/dist-packages (from adanet) (1.17.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.11/dist-packages (from adanet) (1.26.4)\n",
            "Requirement already satisfied: nose<2.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from adanet) (1.3.7)\n",
            "Requirement already satisfied: rednose<2.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from adanet) (1.3.0)\n",
            "Requirement already satisfied: coverage<5.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from adanet) (4.5.4)\n",
            "Requirement already satisfied: protobuf<4.0,>=3.6 in /usr/local/lib/python3.11/dist-packages (from adanet) (3.20.3)\n",
            "Requirement already satisfied: mock<4.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from adanet) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from rednose<2.0,>=1.3->adanet) (75.2.0)\n",
            "Requirement already satisfied: termstyle>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from rednose<2.0,>=1.3->adanet) (0.1.11)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from rednose<2.0,>=1.3->adanet) (0.4.6)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (0.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (1.26.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install adanet\n",
        "!pip install --user --upgrade tensorflow-probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITZ55s-P2AXL"
      },
      "source": [
        "#Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade keras\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "y26BWX77glOw",
        "outputId": "7fe0c354-f1d2-4e9e-e5a1-40fd09159aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "adanet 0.9.0 requires numpy<2.0,>=1.15, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.2.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from absl-py->keras) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.9.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.9.2)\n",
            "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, absl-py, tensorboard, ml-dtypes, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.15.0\n",
            "    Uninstalling absl-py-0.15.0:\n",
            "      Successfully uninstalled absl-py-0.15.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "adanet 0.9.0 requires absl-py<1.0,>=0.7, but you have absl-py 2.2.2 which is incompatible.\n",
            "adanet 0.9.0 requires numpy<2.0,>=1.15, but you have numpy 2.1.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-2.2.2 ml-dtypes-0.5.1 numpy-2.1.3 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "ml_dtypes"
                ]
              },
              "id": "cd8b175dd51849a79311c39f21462ff2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kusq84g_p94T",
        "outputId": "6ad79823-b6e9-4a95-b8c1-0db83427aaa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_estimator'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c83e7fe7e75b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0madanet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/adanet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/adanet/distributed/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplacement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPlacementStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplacement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplicationStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplacement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoundRobinStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/adanet/distributed/placement.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_OpNameHashStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/adanet/tf_compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtpu_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregression_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# pylint: enable=g-direct-tensorflow-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# pylint: enable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_estimator'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import math\n",
        "import tensorflow.compat.v2 as tf\n",
        "import adanet\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing import text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN3TWLjg1v3r"
      },
      "source": [
        "# Upload Dataset\n",
        "\n",
        "Run the following cell and, when prompted, upload files *testing_set.csv*, *training_set.csv*, and *validation_set.csv* (that you downloaded as part of the prerequisites)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9iCyym11vQk"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfkASPTe3VNG"
      },
      "source": [
        "# Load dataset in pandas.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk1ZmWOG3Z4F"
      },
      "outputs": [],
      "source": [
        "training_set = pd.read_csv('training_set.csv', keep_default_na=False)\n",
        "eval_set = pd.read_csv('validation_set.csv', keep_default_na=False)\n",
        "test_set = pd.read_csv('testing_set.csv', keep_default_na=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ9P01jZ_P2c"
      },
      "source": [
        "# Select Model\n",
        "\n",
        "In the next cell you can select which model to train. Remember to run the cell after making a selection. The features each model uses are:\n",
        "\n",
        "Column Name|Type|Contents|Lightweight Model|Full Model\n",
        "-----------|----|-|:-:|:--------:\n",
        "source_url| string |url of a webpage that contains schema.org/Dataset markup| |+\n",
        "name| string |The name of the dataset| +|+\n",
        "description| string |Description of the dataset|+|+\n",
        "has_distribution| bool|True if the dataset contains distribution metadata, false otherwise| |+\n",
        "has_encoding_or_file_format| bool |True if the dataset contains encoding or file format metadata, false otherwise| |+\n",
        "provider_or_publisher| string |The name of the provider or publisher of the dataset| |+\n",
        "author_or_creator| string |The author(s) or creator(s) of the dataset| |+\n",
        "doi| string|The Digital Object Identifier of the dataset| |+\n",
        "has_catalog| bool |True if the dataset is included in a data catalog, false otherwise| |+|\n",
        "has_dateCreated| bool |True if a creation date is provided, false otherwise| |+\n",
        "has_dateModified| bool |True if a modification date is provided, false otherwise| |+\n",
        "has_datePublished| bool |True if a publication date is provided, false otherwise| |+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iGEKT8lk_dQ9"
      },
      "outputs": [],
      "source": [
        "SELECTED_MODEL = 'lightweight_model'  #@param {type:'string'} [\"lightweight_model\", \"full_model\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_etikoRVa8w"
      },
      "source": [
        "# Preprocessing Parameters\n",
        "\n",
        "Dictionary with the sizes of the feature vocabularies to generate during preprocessing for each of the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgWaVoQEVqsK"
      },
      "outputs": [],
      "source": [
        "P_PARAMS_BY_MODEL = {\n",
        "    'lightweight_model': {\n",
        "        'vocab_size_by_feature': {\n",
        "            'description': 110211,\n",
        "            'name': 18720\n",
        "        },\n",
        "        'MAX_TOKENS': 400\n",
        "    },\n",
        "    'full_model': {\n",
        "        'vocab_size_by_feature': {\n",
        "            'description': 104383,\n",
        "            'name': 17495,\n",
        "            'author_or_creator': 1602,\n",
        "            'doi': 193,\n",
        "            'provider_or_publisher': 773,\n",
        "            'source_url': 17749\n",
        "        },\n",
        "        'MAX_TOKENS': 400\n",
        "    }\n",
        "}\n",
        "\n",
        "MODEL_P_PARAMS = P_PARAMS_BY_MODEL[SELECTED_MODEL]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0N9Pf6555ge"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Analyze training dataset and generate tokenizers with custom vocabularies for each text feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-tHyTdn6Fy1"
      },
      "outputs": [],
      "source": [
        "tokenizers = {}\n",
        "\n",
        "for feature_name, vocab_size in MODEL_P_PARAMS['vocab_size_by_feature'].items():\n",
        "  tokenizers[feature_name] = text.Tokenizer(num_words=vocab_size)\n",
        "  tokenizers[feature_name].fit_on_texts(training_set[feature_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5YK0Q1TAaZ3"
      },
      "source": [
        "# Hyperparametes\n",
        "\n",
        "Dictionary with the training hyperparameters for each of the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW2NqYpmAZn5"
      },
      "outputs": [],
      "source": [
        "H_PARAMS_BY_MODEL = {\n",
        "    'lightweight_model': {\n",
        "        'features': ['description', 'name'],\n",
        "        'LEARNING_RATE': 0.00677,\n",
        "        'TRAIN_STEPS': 500,\n",
        "        'SHUFFLE_BUFFER_SIZE': 2048,\n",
        "        'BATCH_SIZE': 128,\n",
        "        'CLIP_NORM': 0.00037,\n",
        "        'HIDDEN_UNITS': [186],\n",
        "        'DROPOUT': 0.28673,\n",
        "        'ACTIVATION_FN': tf.nn.selu,\n",
        "        'MAX_ITERATION_STEPS': 333333,\n",
        "        'DO_BATCH_NORM': True,\n",
        "        'MAX_TRAIN_STEPS': 1000\n",
        "    },\n",
        "    'full_model': {\n",
        "        'features': [\n",
        "            'author_or_creator', 'description', 'doi', 'has_date_created',\n",
        "            'has_date_modified', 'has_date_published', 'has_distribution',\n",
        "            'has_encoding_or_file_format', 'name', 'provider_or_publisher',\n",
        "            'source_url'\n",
        "        ],\n",
        "        'LEARNING_RATE': 0.00076,\n",
        "        'TRAIN_STEPS': 500,\n",
        "        'SHUFFLE_BUFFER_SIZE': 2048,\n",
        "        'BATCH_SIZE': 128,\n",
        "        'CLIP_NORM': 0.25035,\n",
        "        'HIDDEN_UNITS': [329, 351, 292],\n",
        "        'DROPOUT': 0.08277,\n",
        "        'ACTIVATION_FN': tf.nn.selu,\n",
        "        'MAX_ITERATION_STEPS': 333333,\n",
        "        'DO_BATCH_NORM': False,\n",
        "        'MAX_TRAIN_STEPS': 1000\n",
        "    }\n",
        "}\n",
        "\n",
        "MODEL_H_PARAMS = H_PARAMS_BY_MODEL[SELECTED_MODEL]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCtOrKZ41k91"
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "Methods used to preprocess and create the input for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR0-SK5ddFmc"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_pad(features):\n",
        "  \"\"\"Iterates over the features of a labeled sample, tokenizing and padding them.\n",
        "\n",
        "  Args:\n",
        "    features: A dictionary of feature values keyed by feature names. It includes\n",
        "      label as a feature\n",
        "\n",
        "  Returns:\n",
        "    A tuple with the processed features\n",
        "  \"\"\"\n",
        "\n",
        "  tokenized_features = list()\n",
        "  for feature in MODEL_H_PARAMS['features']:\n",
        "    # Tokenize text features according to the corresponding vocabulary\n",
        "    if feature in MODEL_P_PARAMS['vocab_size_by_feature']:\n",
        "      # Handle missing features\n",
        "      if not features[feature]:\n",
        "        tokenized = [[MODEL_P_PARAMS['vocab_size_by_feature'][feature]]]\n",
        "      else:\n",
        "        tokenized = tokenizers[feature].texts_to_sequences([features[feature]])\n",
        "      tokenized_features.append([\n",
        "          sequence.pad_sequences(\n",
        "              tokenized,\n",
        "              maxlen=MODEL_P_PARAMS['MAX_TOKENS'],\n",
        "              padding='post',\n",
        "              truncating='post')\n",
        "      ])\n",
        "    # Tokenize boolean features into binary values\n",
        "    else:\n",
        "      if features[feature]:\n",
        "        tokenized_features.append([1])\n",
        "      else:\n",
        "        tokenized_features.append([0])\n",
        "  tokenized_features.append(features['label'])\n",
        "  return tuple(tokenized_features)\n",
        "\n",
        "\n",
        "def generator(dataset):\n",
        "  \"\"\"Returns a generator mapping dataset entries to tokenized features-label pairs.\"\"\"\n",
        "\n",
        "  def _gen():\n",
        "    for entry in dataset.iterrows():\n",
        "      yield tokenize_and_pad(entry[1])\n",
        "\n",
        "  return _gen\n",
        "\n",
        "\n",
        "def preprocess(*args):\n",
        "  \"\"\"Tensorizes its arguments.\n",
        "\n",
        "  Args:\n",
        "    *args: Variable length arguments feature1, ..., featureK, label. Should be\n",
        "      in the same order as in MODEL_H_PARAMS['features']\n",
        "\n",
        "  Returns:\n",
        "    A pair of\n",
        "      1. A dictionary with the features keyed by their names\n",
        "      2. A label\n",
        "  \"\"\"\n",
        "  m = {}\n",
        "  for feature, name in zip(args[:-1], MODEL_H_PARAMS['features']):\n",
        "    m[name] = feature\n",
        "  return m, [args[-1]]\n",
        "\n",
        "\n",
        "def generate_output_types():\n",
        "  \"\"\"Returns a vector of output types corresponding to the tuple produced by the generator.\"\"\"\n",
        "  types = []\n",
        "  # Feature types\n",
        "  types = [tf.int32] * len(MODEL_H_PARAMS['features'])\n",
        "  # Label type\n",
        "  types.append(tf.bool)\n",
        "  return tuple(types)\n",
        "\n",
        "\n",
        "def input_fn(partition, training, batch_size):\n",
        "  \"\"\"Generates an input_fn for the Estimator.\n",
        "\n",
        "  Args:\n",
        "    partition: One of 'train', 'test', and 'eval' for training, testing, and\n",
        "      validation sets respectively\n",
        "    training: If true, then shuffle dataset to add randomness between epochs\n",
        "    batch_size: Number of elements to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    The input function\n",
        "  \"\"\"\n",
        "\n",
        "  def _input_fn():\n",
        "    if partition == 'train':\n",
        "      dataset = tf.data.Dataset.from_generator(\n",
        "          generator(training_set), generate_output_types())\n",
        "    elif partition == 'test':\n",
        "      dataset = tf.data.Dataset.from_generator(\n",
        "          generator(test_set), generate_output_types())\n",
        "    elif partition == 'eval':\n",
        "      dataset = tf.data.Dataset.from_generator(\n",
        "          generator(eval_set), generate_output_types())\n",
        "    else:\n",
        "      print('Unknown partition')\n",
        "      return\n",
        "\n",
        "    if training:\n",
        "      dataset = dataset.shuffle(MODEL_H_PARAMS['SHUFFLE_BUFFER_SIZE'] *\n",
        "                                batch_size).repeat()\n",
        "\n",
        "    dataset = dataset.map(preprocess).batch(batch_size)\n",
        "    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
        "    features, labels = iterator.get_next()\n",
        "    return features, labels\n",
        "\n",
        "  return _input_fn\n",
        "\n",
        "\n",
        "def generate_feature_columns(embed):\n",
        "  \"\"\"Creates the feature columns that we will train the model on.\n",
        "\n",
        "  Args:\n",
        "    embed: If true, we embed the columns.\n",
        "\n",
        "  Returns:\n",
        "    A list with the feature columns.\n",
        "  \"\"\"\n",
        "  feature_columns = []\n",
        "  for feature in MODEL_H_PARAMS['features']:\n",
        "    if feature in MODEL_P_PARAMS['vocab_size_by_feature']:\n",
        "      # vocab_size + 1 to handle missing features\n",
        "      num_buckets = MODEL_P_PARAMS['vocab_size_by_feature'][feature] + 1\n",
        "    else:\n",
        "      # All none-text features are booleans, so 2 buckets are enough\n",
        "      num_buckets = 2\n",
        "    column = tf.feature_column.categorical_column_with_identity(\n",
        "        key=feature, num_buckets=num_buckets)\n",
        "    if embed:\n",
        "      column = tf.feature_column.embedding_column(\n",
        "          column, dimension=math.ceil(math.log2(num_buckets)))\n",
        "    feature_columns.append(column)\n",
        "  return feature_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUKF4a7R4FWG"
      },
      "source": [
        "# Build Model\n",
        "\n",
        "Set up an ensemble estimator combining a Linear estimator and a DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di00VeBbvXHa"
      },
      "outputs": [],
      "source": [
        "head = tf.estimator.BinaryClassHead()\n",
        "\n",
        "adam = lambda: tf.keras.optimizers.Adam(\n",
        "    learning_rate=MODEL_H_PARAMS['LEARNING_RATE'],\n",
        "    clipnorm=MODEL_H_PARAMS['CLIP_NORM'])\n",
        "\n",
        "estimator = adanet.AutoEnsembleEstimator(\n",
        "    head=head,\n",
        "    candidate_pool={\n",
        "        'linear':\n",
        "            tf.estimator.LinearEstimator(\n",
        "                head=head,\n",
        "                feature_columns=generate_feature_columns(False),\n",
        "                optimizer=adam),\n",
        "        'dnn':\n",
        "            tf.estimator.DNNEstimator(\n",
        "                head=head,\n",
        "                hidden_units=MODEL_H_PARAMS['HIDDEN_UNITS'],\n",
        "                feature_columns=generate_feature_columns(True),\n",
        "                optimizer=adam,\n",
        "                activation_fn=MODEL_H_PARAMS['ACTIVATION_FN'],\n",
        "                dropout=MODEL_H_PARAMS['DROPOUT'],\n",
        "                batch_norm=MODEL_H_PARAMS['DO_BATCH_NORM'])\n",
        "    },\n",
        "    max_iteration_steps=MODEL_H_PARAMS['MAX_ITERATION_STEPS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fb7wW8o4qMm"
      },
      "source": [
        "# Train Model\n",
        "\n",
        "For demonstration purposes, we set *max_steps* to a small value so that the\n",
        "training finishes fast. This is enough to achieve good results. Alternatively, you can remove the *max_steps* argument and let the estimator train to convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX_sbCcZ4oZl"
      },
      "outputs": [],
      "source": [
        "result = tf.estimator.train_and_evaluate(\n",
        "    estimator,\n",
        "    train_spec=tf.estimator.TrainSpec(\n",
        "        input_fn=input_fn(\n",
        "            'train', training=True, batch_size=MODEL_H_PARAMS['BATCH_SIZE']),\n",
        "        max_steps=MODEL_H_PARAMS['MAX_TRAIN_STEPS']),\n",
        "    eval_spec=tf.estimator.EvalSpec(\n",
        "        input_fn=input_fn(\n",
        "            'eval', training=False, batch_size=MODEL_H_PARAMS['BATCH_SIZE']),\n",
        "        steps=None,\n",
        "        start_delay_secs=1,\n",
        "        throttle_secs=1,\n",
        "    ))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLif6fwgZeAJ"
      },
      "source": [
        "# Model perfomance on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdK0LraQgrvH"
      },
      "outputs": [],
      "source": [
        "print('AUC:', result['auc'], 'AUC_PR:', result['auc_precision_recall'],\n",
        "      'Recall:', result['recall'], 'Precision:', result['precision'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RuzrIl_ZxmH"
      },
      "source": [
        "# Model perfomance on testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG0avhs5x2s_"
      },
      "outputs": [],
      "source": [
        "ret = estimator.evaluate(\n",
        "    input_fn('test', training=False, batch_size=MODEL_H_PARAMS['BATCH_SIZE']))\n",
        "print('AUC:', ret['auc'], 'AUC_PR:', ret['auc_precision_recall'], 'Recall:',\n",
        "      ret['recall'], 'Precision:', ret['precision'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "dataset_or_not.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}